{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8167ed90-6bee-4a6d-9c68-e459e5449c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raiymbek/miniconda3/envs/vlm_proper/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2Config\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ViTImageProcessor, ViTModel, ViTConfig, CLIPProcessor, CLIPModel\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from transformers import ConvNextImageProcessor, ConvNextForImageClassification\n",
    "from transformers import AutoImageProcessor, DetrModel\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from transformers import SamModel, SamProcessor\n",
    "from transformers import SegformerModel\n",
    "from transformers import DPTImageProcessor, DPTForDepthEstimation\n",
    "from transformers import AutoFeatureExtractor, ResNetForImageClassification\n",
    "import timm\n",
    "\n",
    "\n",
    "import torch, random\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import re\n",
    "\n",
    "gpu = 'cuda:1'\n",
    "device = torch.device(gpu)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53fb47c2-e713-40e8-9e49-ee78ccab566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.extracted_features = None\n",
    "\n",
    "    def __call__(self, module, input_, output):\n",
    "        self.extracted_features = output\n",
    "\n",
    "def save_embed(model_name, dataset):\n",
    "    \n",
    "    if model_name == \"convnext\":\n",
    "        processor = ConvNextImageProcessor.from_pretrained(\"facebook/convnext-base-224-22k\")\n",
    "        model = ConvNextForImageClassification.from_pretrained(\"facebook/convnext-base-224-22k\").to(device)\n",
    "    elif model_name == \"dinov2\":\n",
    "        vision_model_name = \"facebook/dinov2-large\"\n",
    "        processor = AutoImageProcessor.from_pretrained(vision_model_name)\n",
    "        model = AutoModel.from_pretrained(vision_model_name).to(device)\n",
    "    elif model_name == \"clip\":\n",
    "        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "    elif model_name == \"allroberta\":\n",
    "        language_model_name = \"all-roberta-large-v1\"\n",
    "        language_model = SentenceTransformer(language_model_name).to(device)\n",
    "    elif model_name == \"detr_resnet_50_encoder\":\n",
    "        image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        model = DetrModel.from_pretrained(\"facebook/detr-resnet-50\").to(device)\n",
    "        \n",
    "        extractor = FeatureExtractor()\n",
    "        model.encoder.register_forward_hook(extractor)\n",
    "    elif model_name == \"detr_resnet_50_decoder\":\n",
    "        image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        model = DetrModel.from_pretrained(\"facebook/detr-resnet-50\").to(device)\n",
    "        \n",
    "        extractor = FeatureExtractor()\n",
    "        model.decoder.register_forward_hook(extractor)\n",
    "    elif model_name == \"detr_resnet_50_backbone\":\n",
    "        image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        model = DetrModel.from_pretrained(\"facebook/detr-resnet-50\").to(device)\n",
    "        \n",
    "        extractor = FeatureExtractor()\n",
    "        model.backbone.conv_encoder.register_forward_hook(extractor)\n",
    "    elif model_name == \"detr_resnet_101_backbone\":\n",
    "        image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-101\")\n",
    "        model = DetrModel.from_pretrained(\"facebook/detr-resnet-101\").to(device)\n",
    "        \n",
    "        extractor = FeatureExtractor()\n",
    "        model.backbone.conv_encoder.register_forward_hook(extractor)\n",
    "    elif model_name == \"detr_resnet_101_encoder\":\n",
    "        image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-101\")\n",
    "        model = DetrModel.from_pretrained(\"facebook/detr-resnet-101\").to(device)\n",
    "        \n",
    "        extractor = FeatureExtractor()\n",
    "        model.encoder.register_forward_hook(extractor)\n",
    "    elif model_name == \"detr_resnet_101_decoder\":\n",
    "        image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-101\")\n",
    "        model = DetrModel.from_pretrained(\"facebook/detr-resnet-101\").to(device)\n",
    "        \n",
    "        extractor = FeatureExtractor()\n",
    "        model.decoder.register_forward_hook(extractor)\n",
    "    elif model_name == \"sam\":\n",
    "        model = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(device)\n",
    "        processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n",
    "        \n",
    "        extractor = FeatureExtractor()\n",
    "        model.vision_encoder.layers[31].register_forward_hook(extractor)\n",
    "    elif model_name == \"sam_embed\":\n",
    "        model = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(device)\n",
    "        processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n",
    "        \n",
    "        extractor = FeatureExtractor()\n",
    "        model.shared_image_embedding.register_forward_hook(extractor)\n",
    "    elif model_name == \"segformer\":\n",
    "        processor = AutoImageProcessor.from_pretrained(\"nvidia/mit-b0\")\n",
    "        model = SegformerModel.from_pretrained(\"nvidia/mit-b0\").to(device)\n",
    "\n",
    "        extractor = FeatureExtractor()\n",
    "        model.encoder.register_forward_hook(extractor)\n",
    "    elif model_name == \"segformer_segment\":\n",
    "        processor = AutoImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "        model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\").to(device)\n",
    "\n",
    "        extractor = FeatureExtractor()\n",
    "        model.segformer.encoder.register_forward_hook(extractor)\n",
    "    elif model_name == \"dpt\":\n",
    "        processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-large\")\n",
    "        model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\").to(device)\n",
    "\n",
    "        extractor = FeatureExtractor()\n",
    "        model.dpt.encoder.register_forward_hook(extractor)\n",
    "    elif model_name == \"resnet101\":\n",
    "        feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-101\")\n",
    "        model = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-101\").to(device)\n",
    "\n",
    "        extractor = FeatureExtractor()\n",
    "        model.resnet.encoder.register_forward_hook(extractor)\n",
    "    elif model_name == \"vit\":\n",
    "        model = timm.create_model(\n",
    "            'vit_base_patch16_384.augreg_in1k',\n",
    "            pretrained=True,\n",
    "            num_classes=0,  # remove classifier nn.Linear\n",
    "        ).to(device)\n",
    "        model = model.eval()\n",
    "        \n",
    "        # get model specific transforms (normalization, resize)\n",
    "        data_config = timm.data.resolve_model_data_config(model)\n",
    "        transform = timm.data.create_transform(**data_config, is_training=False)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    if dataset==\"coco\":\n",
    "        cap = dset.CocoCaptions(root = '/datasets/coco_2024-01-04_1601/val2017',\n",
    "                        annFile = '/datasets/coco_2024-01-04_1601/annotations/captions_val2017.json')#,\n",
    "                        #transform=transforms.Compose([\n",
    "                            # transforms.Resize((256,256)), \n",
    "                            # transforms.RandomResizedCrop((224,224)), \n",
    "                            #transforms.PILToTensor()]),)\n",
    "                            # target_transform=select_first_k_captions)\n",
    "    elif dataset==\"nocaps\":\n",
    "        cap = dset.CocoCaptions(root = '/shared/group/openimages/validation',\n",
    "                        annFile = 'nocaps_val_4500_captions.json',\n",
    "                        transform=transforms.Compose([\n",
    "                            #transforms.Resize((256,256)), \n",
    "                            #transforms.RandomResizedCrop((224,224)), \n",
    "                            transforms.PILToTensor()]),)\n",
    "        \n",
    "\n",
    "    if model_name == \"dinov2\":\n",
    "        image_representations = []\n",
    "    \n",
    "        for img, target in tqdm(cap):          \n",
    "            inputs = processor(images=img, return_tensors=\"pt\")\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(**inputs)\n",
    "            image_representation = outputs.last_hidden_state.mean(dim=1).detach().cpu()[0]\n",
    "            image_representations.append(image_representation)\n",
    "        image_representations_tensor = torch.stack(image_representations)\n",
    "        torch.save(image_representations_tensor, f'{dataset}_{model_name}_img.pt')\n",
    "    elif \"resnet101\" == model_name:\n",
    "        image_representations = []\n",
    "        for img, target in tqdm(cap):\n",
    "            inputs = feature_extractor(img, return_tensors=\"pt\")\n",
    "            inputs = inputs.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            output = extractor.extracted_features.last_hidden_state.reshape((1, 2048, -1)).mean(dim=-1).detach().cpu()[0]\n",
    "            image_representations.append(output)\n",
    "        image_representations_tensor = torch.stack(image_representations)\n",
    "        torch.save(image_representations_tensor, f'{dataset}_{model_name}_img.pt')\n",
    "    elif \"detr_resnet_50_backbone\" == model_name or \"detr_resnet_101_backbone\" == model_name:\n",
    "        image_representations = []\n",
    "        for img, target in tqdm(cap):\n",
    "            inputs = image_processor(images=img, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            output = extractor.extracted_features[-1][0].reshape((1, 2048, -1)).mean(dim=-1).detach().cpu()[0]\n",
    "            image_representations.append(output)\n",
    "        image_representations_tensor = torch.stack(image_representations)\n",
    "        torch.save(image_representations_tensor, f'{dataset}_{model_name}_img.pt')\n",
    "    elif \"detr_resnet\" in model_name:\n",
    "        image_representations = []\n",
    "    \n",
    "        for img, target in tqdm(cap):\n",
    "            inputs = image_processor(images=img, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            output = extractor.extracted_features.last_hidden_state.mean(dim=1).detach().cpu()[0]\n",
    "            image_representations.append(output)\n",
    "        image_representations_tensor = torch.stack(image_representations)\n",
    "        torch.save(image_representations_tensor, f'{dataset}_{model_name}_img.pt')\n",
    "    elif model_name == \"vit\":\n",
    "        image_representations = []\n",
    "    \n",
    "        for img, target in tqdm(cap):\n",
    "            input = transform(img).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(input)[0]  # output is (batch_size, num_features) shaped tensor\n",
    "            image_representations.append(output)\n",
    "        image_representations_tensor = torch.stack(image_representations)\n",
    "        torch.save(image_representations_tensor, f'{dataset}_{model_name}_img.pt')\n",
    "    elif model_name == \"sam\":\n",
    "        image_representations = []\n",
    "    \n",
    "        for img, target in tqdm(cap):\n",
    "            inputs = processor(img, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            output = extractor.extracted_features[0].reshape(1, -1, 1280).mean(dim = 1).detach().cpu()[0]\n",
    "            image_representations.append(output)\n",
    "        image_representations_tensor = torch.stack(image_representations)\n",
    "        torch.save(image_representations_tensor, f'{dataset}_{model_name}_img.pt')\n",
    "    elif model_name == \"sam_embed\":\n",
    "        image_representations = []\n",
    "    \n",
    "        for img, target in tqdm(cap):\n",
    "            inputs = processor(img, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            output = extractor.extracted_features.reshape(-1, 256).mean(dim = 0).detach().cpu()\n",
    "            image_representations.append(output)\n",
    "        image_representations_tensor = torch.stack(image_representations)\n",
    "        torch.save(image_representations_tensor, f'{dataset}_{model_name}_img.pt')\n",
    "    elif model_name == \"segformer\" or model_name == \"segformer_segment\":\n",
    "        image_representations = []\n",
    "    \n",
    "        for img, target in tqdm(cap):\n",
    "            inputs = processor(img, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            output = extractor.extracted_features.last_hidden_state.reshape((1, 256, -1)).mean(-1).detach().cpu()[0]\n",
    "            image_representations.append(output)\n",
    "        image_representations_tensor = torch.stack(image_representations)\n",
    "        torch.save(image_representations_tensor, f'{dataset}_{model_name}_img.pt')\n",
    "    elif model_name == \"dpt\":\n",
    "        image_representations = []\n",
    "    \n",
    "        for img, target in tqdm(cap):\n",
    "            inputs = processor(img, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            output = extractor.extracted_features.last_hidden_state.mean(dim=1).detach().cpu()[0]\n",
    "            image_representations.append(output)\n",
    "        image_representations_tensor = torch.stack(image_representations)\n",
    "        torch.save(image_representations_tensor, f'{dataset}_{model_name}_img.pt')\n",
    "    elif model_name == \"convnext\":\n",
    "        print(\"here\")\n",
    "        image_representations = []\n",
    "    \n",
    "        for img, target in tqdm(cap):\n",
    "            \n",
    "            inputs = processor(images=img, return_tensors=\"pt\")\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model.convnext(**inputs)\n",
    "            #print(outputs.last_hidden_state.shape)\n",
    "            image_representation = outputs.last_hidden_state.reshape(1, 1024, -1).mean(2).detach().cpu()[0]\n",
    "            image_representation = image_representation / np.linalg.norm(image_representation, axis=0, keepdims=True)\n",
    "            image_representations.append(image_representation)\n",
    "\n",
    "        image_representations_tensor = torch.stack(image_representations)\n",
    "        torch.save(image_representations_tensor, f'{dataset}_{model_name}_img.pt')\n",
    "    elif model_name == \"clip\":\n",
    "        image_representations = []\n",
    "        text_representations = []\n",
    "        \n",
    "        for img, target in tqdm(cap):\n",
    "            \n",
    "            inputs = processor(text=target, images=img, return_tensors=\"pt\", padding=True)\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(**inputs)\n",
    "            text_representation = outputs.text_embeds.detach().cpu().squeeze()\n",
    "            text_representation = text_representation.mean(dim=0).squeeze()\n",
    "            image_representation = outputs.image_embeds.detach().cpu().squeeze()\n",
    "        \n",
    "            text_representations.append(text_representation)\n",
    "            image_representations.append(image_representation)\n",
    "\n",
    "        text_representations_tensor = torch.stack(text_representations)\n",
    "        image_representations_tensor = torch.stack(image_representations)\n",
    "\n",
    "        torch.save(text_representations_tensor, f'{dataset}_{model_name}_text.pt')\n",
    "        torch.save(image_representations_tensor, f'{dataset}_{model_name}_img.pt')\n",
    "    elif model_name == \"allroberta\":\n",
    "        text_representations = []\n",
    "\n",
    "        for img, target in tqdm(cap):\n",
    "            output = language_model.encode(target)\n",
    "            text_representation = torch.Tensor(output)\n",
    "            text_representation = text_representation.mean(dim=0)\n",
    "            text_representations.append(text_representation)\n",
    "            \n",
    "        text_representations_tensor = torch.stack(text_representations)\n",
    "        torch.save(text_representations_tensor, f'{dataset}_{model_name}_text.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "818eda6a-ed73-4c50-99a4-05cfb650d31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 5000/5000 [01:40<00:00, 49.75it/s]\n"
     ]
    }
   ],
   "source": [
    "for model in [\"vit\"]:\n",
    "    for dataset in [\"coco\"]:\n",
    "        save_embed(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e82f6e64-a649-4ad8-87c1-b6f1fbccbb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 5000/5000 [08:49<00:00,  9.45it/s]\n"
     ]
    }
   ],
   "source": [
    "for model in [\"detr_resnet_101_backbone\"]:\n",
    "    for dataset in [\"coco\"]:\n",
    "        save_embed(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a2fe8f-c37a-44cd-99f0-9435716b8255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ea2ffcc-2161-46e2-8088-e8653be6361c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:37<00:00, 31.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.11s)\n",
      "creating index...\n",
      "index created!\n",
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4500/4500 [03:28<00:00, 21.60it/s]\n"
     ]
    }
   ],
   "source": [
    "for model in [\"convnext\"]:\n",
    "    for dataset in [\"coco\", \"nocaps\"]:\n",
    "        save_embed(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f23c3f-e7b3-4783-b2f0-6f4b3aa52eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f49c405-1ac8-41ec-a85a-7b195e5a15ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95e8f70-5b91-4711-b0b5-c72483be8341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d280ffe-246e-42a4-83a1-4a727aba83c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dba2fc-c258-454a-9c88-3b08940eb6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1bbd3e-ff68-4fa4-8170-5796e9aeac93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a604bf-3f75-4d6f-ac87-d94da30f283d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e412f47-733b-4848-89d2-26d6d5cab8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "598e2b69-5fb8-477f-bc55-fe648cf48af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/5000 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "de = save_embed(\"convnext\", \"coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8bee16d-1c1e-44d3-938b-fa0928e3a452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 7, 7])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59e2ea94-6c4b-45b7-99b5-5e171ec62fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ConvNextImageProcessor.from_pretrained(\"facebook/convnext-base-224-22k\")\n",
    "model = ConvNextForImageClassification.from_pretrained(\"facebook/convnext-base-224-22k\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dad78a-7908-4fab-8989-28a5d453dacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7876830-52c9-40f6-9875-74fd29fd23c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNextModel(\n",
       "  (embeddings): ConvNextEmbeddings(\n",
       "    (patch_embeddings): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (layernorm): ConvNextLayerNorm()\n",
       "  )\n",
       "  (encoder): ConvNextEncoder(\n",
       "    (stages): ModuleList(\n",
       "      (0): ConvNextStage(\n",
       "        (downsampling_layer): Identity()\n",
       "        (layers): Sequential(\n",
       "          (0): ConvNextLayer(\n",
       "            (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): ConvNextLayer(\n",
       "            (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): ConvNextLayer(\n",
       "            (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ConvNextStage(\n",
       "        (downsampling_layer): Sequential(\n",
       "          (0): ConvNextLayerNorm()\n",
       "          (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (layers): Sequential(\n",
       "          (0): ConvNextLayer(\n",
       "            (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): ConvNextLayer(\n",
       "            (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): ConvNextLayer(\n",
       "            (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): ConvNextStage(\n",
       "        (downsampling_layer): Sequential(\n",
       "          (0): ConvNextLayerNorm()\n",
       "          (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (layers): Sequential(\n",
       "          (0): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (3): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (4): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (5): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (6): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (7): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (8): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (9): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (10): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (11): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (12): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (13): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (14): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (15): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (16): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (17): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (18): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (19): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (20): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (21): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (22): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (23): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (24): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (25): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (26): ConvNextLayer(\n",
       "            (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): ConvNextStage(\n",
       "        (downsampling_layer): Sequential(\n",
       "          (0): ConvNextLayerNorm()\n",
       "          (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (layers): Sequential(\n",
       "          (0): ConvNextLayer(\n",
       "            (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (1): ConvNextLayer(\n",
       "            (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "          (2): ConvNextLayer(\n",
       "            (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
       "            (layernorm): ConvNextLayerNorm()\n",
       "            (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELUActivation()\n",
       "            (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.convnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105313fc-c8ae-4f19-9fbb-d9a3a9c4791c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44e3e3f-b8a9-4252-a687-23d489436126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "171a36a9-fc49-429d-b4c6-4094b563e9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "vision_model_name = \"facebook/convnext-base-224-22k\"\n",
    "processor = AutoImageProcessor.from_pretrained(vision_model_name)\n",
    "model = AutoModel.from_pretrained(vision_model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b794b1f-5d14-418f-95b6-7071d865e2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "cap = dset.CocoCaptions(root = '/shared/group/openimages/validation',\n",
    "                        annFile = 'nocaps_val_4500_captions.json',\n",
    "                        transform=transforms.Compose([\n",
    "                            #transforms.Resize((256,256)), \n",
    "                            #transforms.RandomResizedCrop((224,224)), \n",
    "                            transforms.PILToTensor()]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb80016b-133c-4611-a66f-9890bde96a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4500/4500 [03:05<00:00, 24.25it/s]\n"
     ]
    }
   ],
   "source": [
    "image_representations = []\n",
    "\n",
    "for img, target in tqdm(cap):\n",
    "    \n",
    "    inputs = processor(images=img, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model(**inputs)\n",
    "    image_representation = outputs.last_hidden_state.reshape(1, 1024, -1).mean(2).detach().cpu()[0]\n",
    "    image_representations.append(image_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ad9ad30-ecf1-42a3-9a05-0f27fc4ab325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_representation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2ab06bf-b51b-4e00-a84a-175f28f946a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_representations_tensor = torch.stack(image_representations)\n",
    "torch.save(image_representations_tensor, f'convnext_nocaps_val.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12a0b60b-313f-4799-8e74-230687ba6ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "cap = dset.CocoCaptions(root = '/shared/group/coco/val2017',\n",
    "                        annFile = '/shared/group/coco/annotations/captions_val2017.json',\n",
    "                        transform=transforms.Compose([\n",
    "                            # transforms.Resize((256,256)), \n",
    "                            # transforms.RandomResizedCrop((224,224)), \n",
    "                            transforms.PILToTensor()]),)\n",
    "                            # target_transform=select_first_k_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7090330b-e94c-4ecc-83c2-570b4de188c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 5000/5000 [02:25<00:00, 34.32it/s]\n"
     ]
    }
   ],
   "source": [
    "image_representations = []\n",
    "\n",
    "for img, target in tqdm(cap):\n",
    "    \n",
    "    inputs = processor(images=img, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model(**inputs)\n",
    "    image_representation = outputs.last_hidden_state.reshape(1, 1024, -1).mean(2).detach().cpu()[0]\n",
    "    image_representations.append(image_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cc3413d-faf2-4908-9c99-7c3f463dcd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_representations_tensor = torch.stack(image_representations)\n",
    "torch.save(image_representations_tensor, f'convnext_coco_val.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f0aeafa-294a-44bc-81a0-e118635ae36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f19e7b68-bfb4-4503-a970-080b56fde015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "cap = dset.CocoCaptions(root = '/shared/group/coco/val2017',\n",
    "                        annFile = '/shared/group/coco/annotations/captions_val2017.json',\n",
    "                        transform=transforms.Compose([\n",
    "                            # transforms.Resize((256,256)), \n",
    "                            # transforms.RandomResizedCrop((224,224)), \n",
    "                            transforms.PILToTensor()]),)\n",
    "                            # target_transform=select_first_k_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40388e1c-5da5-4e0c-847d-dced2697dce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 5000/5000 [04:42<00:00, 17.70it/s]\n"
     ]
    }
   ],
   "source": [
    "image_representations = []\n",
    "text_representations = []\n",
    "\n",
    "for img, target in tqdm(cap):\n",
    "    \n",
    "    inputs = processor(text=target, images=img, return_tensors=\"pt\", padding=True)\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model(**inputs)\n",
    "    text_representation = outputs.text_embeds.detach().cpu().squeeze()\n",
    "    text_representation = text_representation.mean(dim=0).squeeze()\n",
    "    image_representation = outputs.image_embeds.detach().cpu().squeeze()\n",
    "\n",
    "    text_representations.append(text_representation)\n",
    "    image_representations.append(image_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1cb5c30f-9769-4948-9874-38149ecfaf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_representations_tensor = torch.stack(text_representations)\n",
    "image_representations_tensor = torch.stack(image_representations)\n",
    "\n",
    "torch.save(text_representations_tensor, f'coco_val_clip_vit_large_patch14_text.pt')\n",
    "torch.save(image_representations_tensor, f'coco_val_clip_vit_large_patch14_img.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12e9cdb5-3401-48da-bf8c-5e484b2c8836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "cap = dset.CocoCaptions(root = '/shared/group/openimages/validation',\n",
    "                        annFile = 'nocaps_val_4500_captions.json',\n",
    "                        transform=transforms.Compose([\n",
    "                            #transforms.Resize((256,256)), \n",
    "                            #transforms.RandomResizedCrop((224,224)), \n",
    "                            transforms.PILToTensor()]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9357ad0-cb1c-4d3c-9340-4bcabe5c7ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4500/4500 [05:52<00:00, 12.77it/s]\n"
     ]
    }
   ],
   "source": [
    "image_representations = []\n",
    "text_representations = []\n",
    "\n",
    "for img, target in tqdm(cap):\n",
    "    \n",
    "    inputs = processor(text=target, images=img, return_tensors=\"pt\", padding=True)\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model(**inputs)\n",
    "    text_representation = outputs.text_embeds.detach().cpu().squeeze()\n",
    "    text_representation = text_representation.mean(dim=0).squeeze()\n",
    "    image_representation = outputs.image_embeds.detach().cpu().squeeze()\n",
    "\n",
    "    text_representations.append(text_representation)\n",
    "    image_representations.append(image_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e6c51c85-0fc3-4e75-8448-91b2775d7d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_representations_tensor = torch.stack(text_representations)\n",
    "image_representations_tensor = torch.stack(image_representations)\n",
    "\n",
    "torch.save(text_representations_tensor, f'nocap_val_clip_vit_large_patch14_text.pt')\n",
    "torch.save(image_representations_tensor, f'nocap_val_clip_vit_large_patch14_img.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e008b4b1-8eeb-46ac-81e0-5d97a35a534d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4500, 768])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_representations_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6bff3fe8-40db-4212-9d69-61883620bf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4500, 768])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_representations_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f6ef863a-6b72-4398-afa6-43dd83cebb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 768])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.text_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6f42a55c-818c-4966-b19c-eb936fe913e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 257, 1024])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['vision_model_output'].last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5a2371-72be-479e-821f-8ea69ae13e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9bca4b-8f10-4fc2-b300-53182a44f761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm proper",
   "language": "python",
   "name": "vlm_proper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
